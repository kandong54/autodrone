{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLOv5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kandong54/autodrone/blob/main/model/yolov5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "- [YOLOv5](https://github.com/ultralytics/yolov5)\n",
        "  - [Train Custom Data](https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data)\n",
        "  - [TFLite, ONNX, CoreML, TensorRT Export](https://github.com/ultralytics/yolov5/issues/251)\n",
        "  - [Transfer Learning with Frozen Layers](https://github.com/ultralytics/yolov5/issues/1314)"
      ],
      "metadata": {
        "id": "csZnzNJ7ybdY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNveqeA1KXGy"
      },
      "source": [
        "# Step 1: Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTvDNSILZoN9"
      },
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone --depth 1 https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "\n",
        "# #albumentations\n",
        "# !pip install -q opencv-python-headless==4.5.4.60\n",
        "# !pip install -q -U albumentations\n",
        "# !echo \"$(pip freeze | grep albumentations) is successfully installed\"\n",
        "\n",
        "# Weights & Biases\n",
        "!pip install wandb -qqq\n",
        "import wandb\n",
        "wandb.login()\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zP6USLgz2f0r"
      },
      "source": [
        "# Step 2: Download Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjT5uIHo6l5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download dataset and untar it. The dataset is split into parts due to [googlecolab/colabtools#1915](https://github.com/googlecolab/colabtools/issues/1915).\n",
        "\n",
        "This takes about 15 minutes."
      ],
      "metadata": {
        "id": "pG1UjM5-3QTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf dataset\n",
        "mkdir dataset\n",
        "tar -xf /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/yolov5.tar -C dataset\n",
        "mkdir dataset/images/train\n",
        "tar -xf /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/train0.tar -C dataset/images/train\n",
        "tar -xf /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/train1.tar -C dataset/images/train\n",
        "tar -xf /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/train2.tar -C dataset/images/train\n",
        "tar -xf /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/train3.tar -C dataset/images/train\n",
        "cp /content/drive/Shareddrives/AutoDrone/Datasets/open-images-v6/yolov5/dataset.yaml /content/yolov5/dataset/dataset.yaml"
      ],
      "metadata": {
        "id": "fHUpiDVGO4V_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf runs"
      ],
      "metadata": {
        "id": "XZ59UM6vvnxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7yAi9hd-T4B"
      },
      "source": [
        "# Step 3: Train Our Custom YOLOv5 model\n",
        "\n",
        "Here, we are able to pass a number of arguments:\n",
        "- **img:** define input image size\n",
        "- **batch:** determine batch size, -1 for autobatch\n",
        "- **epochs:** define the number of training epochs. (Note: often, 3000+ are common here!)\n",
        "- **data:** Our dataset locaiton is saved in the `dataset.location`\n",
        "- **weights:** specify a path to weights to start transfer learning from. Here we choose the generic COCO pretrained checkpoint.\n",
        "- **cache:** cache images for faster training\n",
        "- **freeze:** Number of layers to freeze. backbone=10, all=24\n",
        "- **single-cls** train multi-class data as single-class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaFNnxLJbq4J"
      },
      "source": [
        "!python train.py --img 640 --batch 64 --epochs 5 --data dataset/dataset.yaml --weights yolov5n.pt --single-cls # --cache \"disk\" --batch -1 --freeze 24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIRLQOlA14A"
      },
      "source": [
        "# Evaluate Custom YOLOv5 Detector Performance\n",
        "Training losses and performance metrics are saved to Tensorboard and also to a logfile.\n",
        "\n",
        "If you are new to these metrics, the one you want to focus on is `mAP_0.5` - learn more about mean average precision [here](https://blog.roboflow.com/mean-average-precision/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jS9_BxdBBHL"
      },
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Export Model\n",
        "TFlite quantization: float16(default), [int8](https://www.tensorflow.org/lite/performance/post_training_integer_quant)"
      ],
      "metadata": {
        "id": "ox_SpAz3sE2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#export your model's weights for future use\n",
        "\n",
        "from google.colab import files\n",
        "files.download('./runs/train/exp/weights/best.pt')\n",
        "\n",
        "# tflite\n",
        "!python export.py --img 640 --data dataset/dataset.yaml --weights runs/train/exp/weights/best.pt --include tflite # --int8\n",
        "files.download('./runs/train/exp/weights/best-fp16.tflite')\n",
        "# val tflite\n",
        "!python val.py --img 640 --data dataset/dataset.yaml --batch-size 64 --weights runs/train/exp/weights/best-fp16.tflite --single-cls --augment"
      ],
      "metadata": {
        "id": "Gwcbq1Jh39R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtmS7_TXFsT3"
      },
      "source": [
        "#Run Inference  With Trained Weights\n",
        "Run inference with a pretrained checkpoint on contents of `test/images` folder downloaded from Roboflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWjjiBcic3Vz"
      },
      "source": [
        "# !python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source dataset/test/images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbUn4_b9GCKO"
      },
      "source": [
        "# #display inference on ALL test images\n",
        "\n",
        "# import glob\n",
        "# from IPython.display import Image, display\n",
        "\n",
        "# for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "#     display(Image(filename=imageName))\n",
        "#     print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8dHcni6CJYt"
      },
      "source": [
        "# Conclusion and Next Steps\n",
        "\n",
        "Congratulations! You've trained a custom YOLOv5 model to recognize your custom objects.\n",
        "\n",
        "To improve you model's performance, we recommend first interating on your datasets coverage and quality. See this guide for [model performance improvement](https://github.com/ultralytics/yolov5/wiki/Tips-for-Best-Training-Results).\n",
        "\n",
        "To deploy your model to an application, see this guide on [exporting your model to deployment destinations](https://github.com/ultralytics/yolov5/issues/251).\n",
        "\n",
        "Once your model is in production, you will want to continually iterate and improve on your dataset and model via [active learning](https://blog.roboflow.com/what-is-active-learning/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNn-obvOGITm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}